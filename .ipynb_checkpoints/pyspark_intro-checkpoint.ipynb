{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5ff6c8d-afd5-4f3f-9788-0870ca5b6c0f",
   "metadata": {},
   "source": [
    "### Command to print current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf5e4312-664a-4c35-bf78-3a89d37c2bda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E:\\\\Machine Learning A-Z_Download Codes and Datasets\\\\ML_Git'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61cf9a8-ca54-4d47-bd9b-935818372157",
   "metadata": {},
   "source": [
    "### Command to move and copy files from one location to another location"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4b7afb-389e-47b7-b0d5-283f8fb9ad29",
   "metadata": {},
   "source": [
    "In a raw string, backslashes (\\) are treated as literal characters and are not interpreted as escape characters (such as \\n for newline or \\t for tab)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a2d4a12b-4880-4587-8a1e-caca1ade7e8d",
   "metadata": {},
   "source": [
    "import shutil\n",
    "shutil.move(r'E:\\\\Machine Learning A-Z_Download Codes and Datasets\\\\AWS\\pyspark_intro.ipynb'\\\n",
    "            ,r'E:\\\\Machine Learning A-Z_Download Codes and Datasets\\\\ML_Git\\pyspark_intro.ipynb')\n",
    "#shutil.copy(src_path,dst_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a52eb3-bbd3-4a17-997a-1a40941e9e47",
   "metadata": {},
   "source": [
    "### Pyspark Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fff6c5-45d5-4b6b-9259-49cc562ee627",
   "metadata": {},
   "source": [
    "PySpark is a versatile, high-performance tool for anyone working with big data, offering a unified platform for data processing, analytics, machine learning, and real-time applicationsâ€”all accessible through Python\n",
    "\n",
    "Key Uses of PySpark\n",
    "\n",
    "Big Data Processing:\n",
    "PySpark excels at processing and transforming large volumes of structured, semi-structured, or unstructured data. It can handle data at any scale, from a single workstation to thousands of nodes.\n",
    "\n",
    "Data Analysis:\n",
    "You can perform SQL-like queries using PySpark SQL, which allows for interactive data exploration and analysis. PySpark DataFrames provide a familiar tabular interface, similar to Pandas or R, but with distributed computing power.\n",
    "\n",
    "Machine Learning:\n",
    "PySpark integrates with Spark MLlib, allowing users to build, train, and tune machine learning models on massive datasets. This includes tasks like regression, classification, clustering, and recommendation systems.\n",
    "\n",
    "Streaming and Real-Time Analytics:\n",
    "With Spark Streaming, PySpark can process real-time data streams, making it suitable for use cases like fraud detection, monitoring, and alerting.\n",
    "\n",
    "Graph Processing:\n",
    "PySpark supports graph analytics through libraries like GraphFrames, enabling efficient analysis of large-scale graph data (e.g., social networks, recommendation engines).\n",
    "\n",
    "ETL (Extract, Transform, Load):\n",
    "PySpark is often used to build ETL pipelines for data warehousing and data lake solutions. It supports reading from and writing to various data sources and formats, including CSV, Parquet, JSON, and Hive tables.\n",
    "\n",
    "Integration with Other Tools:\n",
    "PySpark can be used alongside popular Python libraries such as Pandas, NumPy, and scikit-learn, and integrates well with Jupyter Notebooks for interactive development.\n",
    "\n",
    "Why Use PySpark?\n",
    "Scalability:\n",
    "Handles petabyte-scale data efficiently, far beyond the limits of single-machine tools.\n",
    "\n",
    "Speed:\n",
    "In-memory computation and optimized execution plans make PySpark significantly faster than disk-based frameworks like Hadoop MapReduce.\n",
    "\n",
    "Fault Tolerance:\n",
    "Built-in mechanisms for recovering from failures ensure reliability in distributed environments.\n",
    "\n",
    "Flexibility:\n",
    "Supports batch and streaming data, structured and unstructured formats, and a wide range of analytics and machine learning tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006977a3-f673-4720-81b1-7df55077e5ad",
   "metadata": {},
   "source": [
    "Installing Pyspark in Windows\n",
    "1. Created Python virtual environment named 'my_venv'  cmd: python -m venv my_venv\n",
    "2. activate the environment cmd: .\\my_venv\\Scripts\\activate\n",
    "3. install pyspark cmd: pip install pyspark\n",
    "4. install terminal for pyspark cmd: pip install ipykernel\n",
    "5. install the kernel cmd: python -m ipykernel install --user --name PYSPARK_KERNEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "054891ae-22bd-490d-b612-dd2fc4722f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "# if got the error of PySparkRuntimeError: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number. \n",
    "import os\n",
    "os.environ['JAVA_HOME'] = r'C:\\Users\\sainadh\\my_venv\\Java\\jdk-21'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95f5385c-42bf-4271-a9bd-8d78f2c14f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fc930f8-af52-4042-bda2-46d05af7bccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing spark session\n",
    "spark=SparkSession.builder.getOrCreate()\n",
    "\n",
    "file_path=os.getcwd()+r'\\BigMart_Sales.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a372cd7-8d08-4648-8368-30137af284dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Item_Identifier: string (nullable = true)\n",
      " |-- Item_Weight: string (nullable = true)\n",
      " |-- Item_Fat_Content: string (nullable = true)\n",
      " |-- Item_Visibility: string (nullable = true)\n",
      " |-- Item_Type: string (nullable = true)\n",
      " |-- Item_MRP: string (nullable = true)\n",
      " |-- Outlet_Identifier: string (nullable = true)\n",
      " |-- Outlet_Establishment_Year: string (nullable = true)\n",
      " |-- Outlet_Size: string (nullable = true)\n",
      " |-- Outlet_Location_Type: string (nullable = true)\n",
      " |-- Outlet_Type: string (nullable = true)\n",
      " |-- Item_Outlet_Sales: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read data into pyspark with different file formats\n",
    "df1 = spark.read.format('csv')\\\n",
    "                .option('infershcema',True)\\\n",
    "                .option('header',True)\\\n",
    "                .load(file_path)\n",
    "# inferSchema=True tells PySpark to automatically detect (infer) the data types of each column in your CSV file.\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3482599-a169-4a8a-a9ea-5faffa4ae418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Item_Identifier: string (nullable = true)\n",
      " |-- Item_Weight: string (nullable = true)\n",
      " |-- Item_Fat_Content: string (nullable = true)\n",
      " |-- Item_Visibility: string (nullable = true)\n",
      " |-- Item_Type: string (nullable = true)\n",
      " |-- Item_MRP: string (nullable = true)\n",
      " |-- Outlet_Identifier: string (nullable = true)\n",
      " |-- Outlet_Establishment_Year: string (nullable = true)\n",
      " |-- Outlet_Size: string (nullable = true)\n",
      " |-- Outlet_Location_Type: string (nullable = true)\n",
      " |-- Outlet_Type: string (nullable = true)\n",
      " |-- Item_Outlet_Sales: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# simple code to read the csv data\n",
    "df = spark.read.csv(file_path,header=True)\n",
    "df.printSchema() # returns schema of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5965d9f1-72c0-432f-944e-8cb1d83249da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------------------\n",
      " Item_Identifier           | FDA15             \n",
      " Item_Weight               | 9.3               \n",
      " Item_Fat_Content          | Low Fat           \n",
      " Item_Visibility           | 0.016047301       \n",
      " Item_Type                 | Dairy             \n",
      " Item_MRP                  | 249.8092          \n",
      " Outlet_Identifier         | OUT049            \n",
      " Outlet_Establishment_Year | 1999              \n",
      " Outlet_Size               | Medium            \n",
      " Outlet_Location_Type      | Tier 1            \n",
      " Outlet_Type               | Supermarket Type1 \n",
      " Item_Outlet_Sales         | 3735.138          \n",
      "only showing top 1 row\n"
     ]
    }
   ],
   "source": [
    "# n- no.of rows to be  printed and vertical- Each row will be printed in vertical fashion\n",
    "df.show(n=1, truncate=False, vertical=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b18d3532-01af-4826-8b63-100d797c19d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Item_Identifier</th>\n",
       "      <th>Item_Weight</th>\n",
       "      <th>Item_Fat_Content</th>\n",
       "      <th>Item_Visibility</th>\n",
       "      <th>Item_Type</th>\n",
       "      <th>Item_MRP</th>\n",
       "      <th>Outlet_Identifier</th>\n",
       "      <th>Outlet_Establishment_Year</th>\n",
       "      <th>Outlet_Size</th>\n",
       "      <th>Outlet_Location_Type</th>\n",
       "      <th>Outlet_Type</th>\n",
       "      <th>Item_Outlet_Sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FDA15</td>\n",
       "      <td>9.3</td>\n",
       "      <td>Low Fat</td>\n",
       "      <td>0.016047301</td>\n",
       "      <td>Dairy</td>\n",
       "      <td>249.8092</td>\n",
       "      <td>OUT049</td>\n",
       "      <td>1999</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Tier 1</td>\n",
       "      <td>Supermarket Type1</td>\n",
       "      <td>3735.138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DRC01</td>\n",
       "      <td>5.92</td>\n",
       "      <td>Regular</td>\n",
       "      <td>0.019278216</td>\n",
       "      <td>Soft Drinks</td>\n",
       "      <td>48.2692</td>\n",
       "      <td>OUT018</td>\n",
       "      <td>2009</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Tier 3</td>\n",
       "      <td>Supermarket Type2</td>\n",
       "      <td>443.4228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FDN15</td>\n",
       "      <td>17.5</td>\n",
       "      <td>Low Fat</td>\n",
       "      <td>0.016760075</td>\n",
       "      <td>Meat</td>\n",
       "      <td>141.618</td>\n",
       "      <td>OUT049</td>\n",
       "      <td>1999</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Tier 1</td>\n",
       "      <td>Supermarket Type1</td>\n",
       "      <td>2097.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>FDX07</td>\n",
       "      <td>19.2</td>\n",
       "      <td>Regular</td>\n",
       "      <td>0</td>\n",
       "      <td>Fruits and Vegetables</td>\n",
       "      <td>182.095</td>\n",
       "      <td>OUT010</td>\n",
       "      <td>1998</td>\n",
       "      <td>None</td>\n",
       "      <td>Tier 3</td>\n",
       "      <td>Grocery Store</td>\n",
       "      <td>732.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NCD19</td>\n",
       "      <td>8.93</td>\n",
       "      <td>Low Fat</td>\n",
       "      <td>0</td>\n",
       "      <td>Household</td>\n",
       "      <td>53.8614</td>\n",
       "      <td>OUT013</td>\n",
       "      <td>1987</td>\n",
       "      <td>High</td>\n",
       "      <td>Tier 3</td>\n",
       "      <td>Supermarket Type1</td>\n",
       "      <td>994.7052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FDP36</td>\n",
       "      <td>10.395</td>\n",
       "      <td>Regular</td>\n",
       "      <td>0</td>\n",
       "      <td>Baking Goods</td>\n",
       "      <td>51.4008</td>\n",
       "      <td>OUT018</td>\n",
       "      <td>2009</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Tier 3</td>\n",
       "      <td>Supermarket Type2</td>\n",
       "      <td>556.6088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>FDO10</td>\n",
       "      <td>13.65</td>\n",
       "      <td>Regular</td>\n",
       "      <td>0.012741089</td>\n",
       "      <td>Snack Foods</td>\n",
       "      <td>57.6588</td>\n",
       "      <td>OUT013</td>\n",
       "      <td>1987</td>\n",
       "      <td>High</td>\n",
       "      <td>Tier 3</td>\n",
       "      <td>Supermarket Type1</td>\n",
       "      <td>343.5528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>FDP10</td>\n",
       "      <td>None</td>\n",
       "      <td>Low Fat</td>\n",
       "      <td>0.127469857</td>\n",
       "      <td>Snack Foods</td>\n",
       "      <td>107.7622</td>\n",
       "      <td>OUT027</td>\n",
       "      <td>1985</td>\n",
       "      <td>Medium</td>\n",
       "      <td>Tier 3</td>\n",
       "      <td>Supermarket Type3</td>\n",
       "      <td>4022.7636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>FDH17</td>\n",
       "      <td>16.2</td>\n",
       "      <td>Regular</td>\n",
       "      <td>0.016687114</td>\n",
       "      <td>Frozen Foods</td>\n",
       "      <td>96.9726</td>\n",
       "      <td>OUT045</td>\n",
       "      <td>2002</td>\n",
       "      <td>None</td>\n",
       "      <td>Tier 2</td>\n",
       "      <td>Supermarket Type1</td>\n",
       "      <td>1076.5986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>FDU28</td>\n",
       "      <td>19.2</td>\n",
       "      <td>Regular</td>\n",
       "      <td>0.09444959</td>\n",
       "      <td>Frozen Foods</td>\n",
       "      <td>187.8214</td>\n",
       "      <td>OUT017</td>\n",
       "      <td>2007</td>\n",
       "      <td>None</td>\n",
       "      <td>Tier 2</td>\n",
       "      <td>Supermarket Type1</td>\n",
       "      <td>4710.535</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Item_Identifier Item_Weight Item_Fat_Content Item_Visibility  \\\n",
       "0           FDA15         9.3          Low Fat     0.016047301   \n",
       "1           DRC01        5.92          Regular     0.019278216   \n",
       "2           FDN15        17.5          Low Fat     0.016760075   \n",
       "3           FDX07        19.2          Regular               0   \n",
       "4           NCD19        8.93          Low Fat               0   \n",
       "5           FDP36      10.395          Regular               0   \n",
       "6           FDO10       13.65          Regular     0.012741089   \n",
       "7           FDP10        None          Low Fat     0.127469857   \n",
       "8           FDH17        16.2          Regular     0.016687114   \n",
       "9           FDU28        19.2          Regular      0.09444959   \n",
       "\n",
       "               Item_Type  Item_MRP Outlet_Identifier  \\\n",
       "0                  Dairy  249.8092            OUT049   \n",
       "1            Soft Drinks   48.2692            OUT018   \n",
       "2                   Meat   141.618            OUT049   \n",
       "3  Fruits and Vegetables   182.095            OUT010   \n",
       "4              Household   53.8614            OUT013   \n",
       "5           Baking Goods   51.4008            OUT018   \n",
       "6            Snack Foods   57.6588            OUT013   \n",
       "7            Snack Foods  107.7622            OUT027   \n",
       "8           Frozen Foods   96.9726            OUT045   \n",
       "9           Frozen Foods  187.8214            OUT017   \n",
       "\n",
       "  Outlet_Establishment_Year Outlet_Size Outlet_Location_Type  \\\n",
       "0                      1999      Medium               Tier 1   \n",
       "1                      2009      Medium               Tier 3   \n",
       "2                      1999      Medium               Tier 1   \n",
       "3                      1998        None               Tier 3   \n",
       "4                      1987        High               Tier 3   \n",
       "5                      2009      Medium               Tier 3   \n",
       "6                      1987        High               Tier 3   \n",
       "7                      1985      Medium               Tier 3   \n",
       "8                      2002        None               Tier 2   \n",
       "9                      2007        None               Tier 2   \n",
       "\n",
       "         Outlet_Type Item_Outlet_Sales  \n",
       "0  Supermarket Type1          3735.138  \n",
       "1  Supermarket Type2          443.4228  \n",
       "2  Supermarket Type1           2097.27  \n",
       "3      Grocery Store            732.38  \n",
       "4  Supermarket Type1          994.7052  \n",
       "5  Supermarket Type2          556.6088  \n",
       "6  Supermarket Type1          343.5528  \n",
       "7  Supermarket Type3         4022.7636  \n",
       "8  Supermarket Type1         1076.5986  \n",
       "9  Supermarket Type1          4710.535  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert a small DataFrame to Pandas for richer display\n",
    "import pandas as pd\n",
    "df.limit(10).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9cd6deb-9eb4-4ebd-8ef5-f63853b82bec",
   "metadata": {},
   "source": [
    "df.display() --> this command is used for rich display of df same like Pandas but it's only works with databricks application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97514514-6b1c-4478-9aa3-7632b2aca629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+----------------+\n",
      "|Item_Identifier|Item_Weight|Item_Fat_Content|\n",
      "+---------------+-----------+----------------+\n",
      "|          FDA15|        9.3|         Low Fat|\n",
      "|          DRC01|       5.92|         Regular|\n",
      "|          FDN15|       17.5|         Low Fat|\n",
      "|          FDX07|       19.2|         Regular|\n",
      "|          NCD19|       8.93|         Low Fat|\n",
      "+---------------+-----------+----------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# getting only few rows of the dataframe\n",
    "df['Item_Identifier','Item_Weight','Item_Fat_Content'].show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c53cff-a4b5-4186-b341-33b7c768d249",
   "metadata": {},
   "source": [
    "### Schme Definitions\n",
    "Manually defining data types of each columns in the dataframe\n",
    "2 ways 1. DDL Schema 2. StructType() Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fedd4778-d271-452c-b186-e24c8492eadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Item_Identifier: string (nullable = true)\n",
      " |-- Item_Weight: string (nullable = true)\n",
      " |-- Item_Fat_Content: string (nullable = true)\n",
      " |-- Item_Visibility: double (nullable = true)\n",
      " |-- Item_Type: string (nullable = true)\n",
      " |-- Item_MRP: double (nullable = true)\n",
      " |-- Outlet_Identifier: string (nullable = true)\n",
      " |-- Outlet_Establishment_Year: integer (nullable = true)\n",
      " |-- Outlet_Size: string (nullable = true)\n",
      " |-- Outlet_Location_Type: string (nullable = true)\n",
      " |-- Outlet_Type: string (nullable = true)\n",
      " |-- Item_Outlet_Sales: double (nullable = true)\n",
      "\n",
      "-RECORD 0--------------------------------------\n",
      " Item_Identifier           | FDA15             \n",
      " Item_Weight               | 9.3               \n",
      " Item_Fat_Content          | Low Fat           \n",
      " Item_Visibility           | 0.016047301       \n",
      " Item_Type                 | Dairy             \n",
      " Item_MRP                  | 249.8092          \n",
      " Outlet_Identifier         | OUT049            \n",
      " Outlet_Establishment_Year | 1999              \n",
      " Outlet_Size               | Medium            \n",
      " Outlet_Location_Type      | Tier 1            \n",
      " Outlet_Type               | Supermarket Type1 \n",
      " Item_Outlet_Sales         | 3735.138          \n",
      "only showing top 1 row\n"
     ]
    }
   ],
   "source": [
    "# DDL schema: we need to define for every column of the dataframe if we define for only one column it'll throw error as below \n",
    "# ParseException: [PARSE_SYNTAX_ERROR] Syntax error at or near end of input. SQLSTATE: 42601 (line 4, pos 0) \n",
    "my_ddl_schema= '''\n",
    "                    Item_Identifier STRING,\n",
    "                    Item_Weight STRING,\n",
    "                    Item_Fat_Content STRING, \n",
    "                    Item_Visibility DOUBLE,\n",
    "                    Item_Type STRING,\n",
    "                    Item_MRP DOUBLE,\n",
    "                    Outlet_Identifier STRING,\n",
    "                    Outlet_Establishment_Year INT,\n",
    "                    Outlet_Size STRING,\n",
    "                    Outlet_Location_Type STRING, \n",
    "                    Outlet_Type STRING,\n",
    "                    Item_Outlet_Sales DOUBLE\n",
    "'''\n",
    "df = spark.read.csv(file_path,header=True,schema=my_ddl_schema)\n",
    "df.printSchema()\n",
    "\n",
    "df.show(n=1,truncate=False,vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "519ee97b-cea0-4a54-abf7-b81616e28e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Item_Identifier: string (nullable = true)\n",
      " |-- Item_Weight: double (nullable = true)\n",
      " |-- Item_Fat_Content: string (nullable = true)\n",
      " |-- Item_Visibility: string (nullable = true)\n",
      " |-- Item_MRP: string (nullable = true)\n",
      " |-- Outlet_Identifier: string (nullable = true)\n",
      " |-- Outlet_Establishment_Year: string (nullable = true)\n",
      " |-- Outlet_Size: string (nullable = true)\n",
      " |-- Outlet_Location_Type: string (nullable = true)\n",
      " |-- Outlet_Type: string (nullable = true)\n",
      " |-- Item_Outlet_Sales: string (nullable = true)\n",
      "\n",
      "-RECORD 0--------------------------------------\n",
      " Item_Identifier           | FDA15             \n",
      " Item_Weight               | 9.3               \n",
      " Item_Fat_Content          | Low Fat           \n",
      " Item_Visibility           | 0.016047301       \n",
      " Item_MRP                  | Dairy             \n",
      " Outlet_Identifier         | 249.8092          \n",
      " Outlet_Establishment_Year | OUT049            \n",
      " Outlet_Size               | 1999              \n",
      " Outlet_Location_Type      | Medium            \n",
      " Outlet_Type               | Tier 1            \n",
      " Item_Outlet_Sales         | Supermarket Type1 \n",
      "only showing top 1 row\n"
     ]
    }
   ],
   "source": [
    "# structtype function contains structfield function, we need to define inside this (col_name,datatyoe as function,nullable=True/False)\n",
    "# nullable means the column can have null values or not\n",
    "my_struct_schema = StructType([ StructField('Item_Identifier',StringType(),True),\n",
    "                              StructField('Item_Weight',DoubleType(),True), \n",
    "                              StructField('Item_Fat_Content',StringType(),True), \n",
    "                              StructField('Item_Visibility',StringType(),True), \n",
    "                              StructField('Item_MRP',StringType(),True), \n",
    "                              StructField('Outlet_Identifier',StringType(),True), \n",
    "                              StructField('Outlet_Establishment_Year',StringType(),True), \n",
    "                              StructField('Outlet_Size',StringType(),True), \n",
    "                              StructField('Outlet_Location_Type',StringType(),True), \n",
    "                              StructField('Outlet_Type',StringType(),True), \n",
    "                              StructField('Item_Outlet_Sales',StringType(),True)\n",
    "\n",
    "])\n",
    "# command to read data with customschema\n",
    "df = spark.read.format('csv')\\\n",
    "                .option('header',True)\\\n",
    "                .schema(my_struct_schema)\\\n",
    "                .load(file_path)\n",
    "df.printSchema()\n",
    "df.show(n=1,vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a36fa04d-5585-4f62-85a2-4a0164a97bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Item_Weight: string (nullable = true)\n",
      "\n",
      "+-----------+\n",
      "|Item_Weight|\n",
      "+-----------+\n",
      "|      FDA15|\n",
      "+-----------+\n",
      "only showing top 1 row\n"
     ]
    }
   ],
   "source": [
    "# if you try to define schema for a single column  it won't throw an error instead \n",
    "# it'll select the first column even though if we mention another column name and changes it's name and data as an example below\n",
    "my_struct_schema=StructType([StructField('Item_Weight',StringType(),True)])\n",
    "df1= spark.read.csv(file_path,header=True,schema=my_struct_schema)\n",
    "df1.printSchema()\n",
    "df1.show(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdca070-dda5-4012-b012-9b1a671dac89",
   "metadata": {},
   "source": [
    "### Fetching few columns from the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d33dcf1b-202a-4455-9e4f-63d5bf131b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+----------------+\n",
      "|Item_Identifier|Item_Weight|Item_Fat_Content|\n",
      "+---------------+-----------+----------------+\n",
      "|          FDA15|        9.3|         Low Fat|\n",
      "|          DRC01|       5.92|         Regular|\n",
      "|          FDN15|       17.5|         Low Fat|\n",
      "|          FDX07|       19.2|         Regular|\n",
      "|          NCD19|       8.93|         Low Fat|\n",
      "+---------------+-----------+----------------+\n",
      "only showing top 5 rows\n",
      "+---------------+-----------+----------------+\n",
      "|Item_Identifier|Item_Weight|Item_Fat_Content|\n",
      "+---------------+-----------+----------------+\n",
      "|          FDA15|        9.3|         Low Fat|\n",
      "|          DRC01|       5.92|         Regular|\n",
      "|          FDN15|       17.5|         Low Fat|\n",
      "|          FDX07|       19.2|         Regular|\n",
      "|          NCD19|       8.93|         Low Fat|\n",
      "+---------------+-----------+----------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# by using select function\n",
    "df.select('Item_Identifier','Item_Weight','Item_Fat_Content').show(n=5)\n",
    "# by simply performing slicing operation\n",
    "df['Item_Identifier','Item_Weight','Item_Fat_Content'].show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886bff25-1b1f-4241-b63d-98a9026f813b",
   "metadata": {},
   "source": [
    "### Data Writing/Exporting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4285c47-667e-426b-8e50-cc0e5572c5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Py4JJavaError: An error occurred while calling o370.save.\n",
    ": java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://cwiki.apache.org/confluence/display/HADOOP2/WindowsProblems\n",
    "it's saving as a folder instead of file\n",
    "'''\n",
    "os.environ['HADOOP_HOME'] = r'C:\\Users\\sainadh\\my_venv\\Hadoop\\bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adf7a933-1cc2-43d8-b4ff-ec0c1a0b05e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o114.csv.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://cwiki.apache.org/confluence/display/HADOOP2/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:789)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:298)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:314)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:1116)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:798)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:838)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:810)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:837)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:810)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:837)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:810)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:988)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:268)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:306)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:189)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:195)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:117)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:115)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:129)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)\r\n\tat scala.util.Try$.apply(Try.scala:217)\r\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\r\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\r\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:241)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:118)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:426)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\r\n\t\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:789)\r\n\t\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:298)\r\n\t\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:314)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:1116)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:798)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:838)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:810)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:837)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:810)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:837)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:810)\r\n\t\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:988)\r\n\t\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\t\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:190)\r\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:268)\r\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:306)\r\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:189)\r\n\t\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:195)\r\n\t\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:117)\r\n\t\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:115)\r\n\t\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:129)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\r\n\t\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\r\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)\r\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\r\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\r\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)\r\n\t\tat scala.util.Try$.apply(Try.scala:217)\r\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\r\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\r\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\r\n\t\t... 20 more\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://cwiki.apache.org/confluence/display/HADOOP2/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:601)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:622)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:645)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:742)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1954)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1912)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1885)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$install$1(ShutdownHookManager.scala:194)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n\tat scala.Option.fold(Option.scala:263)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:195)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:55)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:53)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:159)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala:63)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:250)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:99)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:379)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:961)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:204)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:227)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:96)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1132)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1141)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:521)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:492)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:569)\r\n\t... 27 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcoalesce\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\my_venv\\Lib\\site-packages\\pyspark\\sql\\readwriter.py:2146\u001b[39m, in \u001b[36mDataFrameWriter.csv\u001b[39m\u001b[34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[39m\n\u001b[32m   2127\u001b[39m \u001b[38;5;28mself\u001b[39m.mode(mode)\n\u001b[32m   2128\u001b[39m \u001b[38;5;28mself\u001b[39m._set_opts(\n\u001b[32m   2129\u001b[39m     compression=compression,\n\u001b[32m   2130\u001b[39m     sep=sep,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2144\u001b[39m     lineSep=lineSep,\n\u001b[32m   2145\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2146\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\my_venv\\Lib\\site-packages\\py4j\\java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\my_venv\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:282\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpy4j\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    284\u001b[39m     converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\my_venv\\Lib\\site-packages\\py4j\\protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    325\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    332\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    333\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o114.csv.\n: java.lang.RuntimeException: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://cwiki.apache.org/confluence/display/HADOOP2/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:789)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:298)\r\n\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:314)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:1116)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:798)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:838)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:810)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:837)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:810)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:837)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:810)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:988)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:190)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:268)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:306)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:189)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:195)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:117)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:115)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:129)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)\r\n\tat scala.util.Try$.apply(Try.scala:217)\r\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\r\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1439)\r\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:241)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:118)\r\n\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:426)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n\tSuppressed: org.apache.spark.util.Utils$OriginalTryStackTraceException: Full stacktrace of original doTryWithCallerStacktrace caller\r\n\t\tat org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:789)\r\n\t\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:298)\r\n\t\tat org.apache.hadoop.util.Shell.getSetPermissionCommand(Shell.java:314)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:1116)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkOneDirWithMode(RawLocalFileSystem.java:798)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:838)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:810)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:837)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:810)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirsWithOptionalPermission(RawLocalFileSystem.java:837)\r\n\t\tat org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:810)\r\n\t\tat org.apache.hadoop.fs.ChecksumFileSystem.mkdirs(ChecksumFileSystem.java:988)\r\n\t\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.setupJob(FileOutputCommitter.java:356)\r\n\t\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.setupJob(HadoopMapReduceCommitProtocol.scala:190)\r\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:268)\r\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:306)\r\n\t\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:189)\r\n\t\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:195)\r\n\t\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:117)\r\n\t\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:115)\r\n\t\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:129)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\r\n\t\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\t\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\r\n\t\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\r\n\t\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)\r\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\r\n\t\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\r\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\t\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\t\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)\r\n\t\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)\r\n\t\tat scala.util.Try$.apply(Try.scala:217)\r\n\t\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\r\n\t\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\r\n\t\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\r\n\t\t... 20 more\r\nCaused by: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://cwiki.apache.org/confluence/display/HADOOP2/WindowsProblems\r\n\tat org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:601)\r\n\tat org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:622)\r\n\tat org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:645)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:742)\r\n\tat org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDurationHelper(Configuration.java:1954)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1912)\r\n\tat org.apache.hadoop.conf.Configuration.getTimeDuration(Configuration.java:1885)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.getShutdownTimeout(ShutdownHookManager.java:183)\r\n\tat org.apache.hadoop.util.ShutdownHookManager$HookEntry.<init>(ShutdownHookManager.java:207)\r\n\tat org.apache.hadoop.util.ShutdownHookManager.addShutdownHook(ShutdownHookManager.java:304)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.$anonfun$install$1(ShutdownHookManager.scala:194)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n\tat scala.Option.fold(Option.scala:263)\r\n\tat org.apache.spark.util.SparkShutdownHookManager.install(ShutdownHookManager.scala:195)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks$lzycompute(ShutdownHookManager.scala:55)\r\n\tat org.apache.spark.util.ShutdownHookManager$.shutdownHooks(ShutdownHookManager.scala:53)\r\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:159)\r\n\tat org.apache.spark.util.ShutdownHookManager$.<clinit>(ShutdownHookManager.scala:63)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:250)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir(SparkFileUtils.scala:103)\r\n\tat org.apache.spark.util.SparkFileUtils.createTempDir$(SparkFileUtils.scala:102)\r\n\tat org.apache.spark.util.Utils$.createTempDir(Utils.scala:99)\r\n\tat org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:379)\r\n\tat org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:961)\r\n\tat org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:204)\r\n\tat org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:227)\r\n\tat org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:96)\r\n\tat org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1132)\r\n\tat org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1141)\r\n\tat org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)\r\nCaused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:521)\r\n\tat org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:492)\r\n\tat org.apache.hadoop.util.Shell.<clinit>(Shell.java:569)\r\n\t... 27 more\r\n"
     ]
    }
   ],
   "source": [
    "df.coalesce(1).write.csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a7c119-7ff1-4dde-9b11-58883fe59f73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PYSPARK_KERNEL",
   "language": "python",
   "name": "pyspark_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
